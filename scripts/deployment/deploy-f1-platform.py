#!/usr/bin/env python3
"""
CloudFormation Deployment Script for F1 Data Platform
Modern AWS Serverless Architecture

This script deploys the complete F1 data platform infrastructure using AWS CloudFormation.
It handles stack dependencies and provides deployment status monitoring.
"""

import boto3
import json
import time
import argparse
from typing import Dict, List, Optional
from pathlib import Path


class CloudFormationDeployer:
    """Handles CloudFormation stack deployments with dependency management."""
    
    def __init__(self, region: str = 'us-east-1', profile: Optional[str] = None):
        """Initialize the deployer with AWS configuration."""
        self.region = region
        self.session = boto3.Session(profile_name=profile) if profile else boto3.Session()
        self.cf_client = self.session.client('cloudformation', region_name=region)
        self.s3_client = self.session.client('s3', region_name=region)
        
    def create_deployment_bucket(self, bucket_name: str) -> str:
        """Create S3 bucket for CloudFormation templates if it doesn't exist."""
        try:
            # Check if bucket exists
            self.s3_client.head_bucket(Bucket=bucket_name)
            print(f"‚úì Deployment bucket '{bucket_name}' already exists")
            
            # Enable versioning if not already enabled
            try:
                versioning = self.s3_client.get_bucket_versioning(Bucket=bucket_name)
                if versioning.get('Status') != 'Enabled':
                    self.s3_client.put_bucket_versioning(
                        Bucket=bucket_name,
                        VersioningConfiguration={'Status': 'Enabled'}
                    )
                    print(f"‚úì Enabled versioning on bucket '{bucket_name}'")
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not enable versioning: {str(e)}")
                
        except:
            # Create bucket
            if self.region == 'us-east-1':
                self.s3_client.create_bucket(Bucket=bucket_name)
            else:
                self.s3_client.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': self.region}
                )
            
            # Enable versioning on new bucket
            self.s3_client.put_bucket_versioning(
                Bucket=bucket_name,
                VersioningConfiguration={'Status': 'Enabled'}
            )
            print(f"‚úì Created deployment bucket with versioning: {bucket_name}")
            
        return bucket_name
    
    def upload_template(self, template_path: Path, bucket_name: str, key_prefix: str = "") -> str:
        """Upload CloudFormation template to S3."""
        key = f"{key_prefix}{template_path.name}" if key_prefix else template_path.name
        
        with open(template_path, 'r') as f:
            template_content = f.read()
            
        self.s3_client.put_object(
            Bucket=bucket_name,
            Key=key,
            Body=template_content,
            ContentType='text/yaml'
        )
        
        template_url = f"https://{bucket_name}.s3.{self.region}.amazonaws.com/{key}"
        print(f"‚úì Uploaded template: {template_path.name}")
        return template_url
    
    def deploy_stack(self, stack_name: str, template_url: str, parameters: Dict[str, str], 
                    capabilities: List[str] = None) -> bool:
        """Deploy or update a CloudFormation stack."""
        
        # Convert parameters to CloudFormation format
        cf_parameters = [
            {"ParameterKey": key, "ParameterValue": value}
            for key, value in parameters.items()
        ]
        
        # Check if stack exists
        stack_exists = self._stack_exists(stack_name)
        
        try:
            if stack_exists:
                # Check if we need to recreate the stack due to parameter changes
                if self._needs_recreation(stack_name, parameters):
                    print(f"üîÑ Stack parameters changed - deleting and recreating: {stack_name}")
                    self._delete_stack(stack_name)
                    stack_exists = False
                else:
                    print(f"üìù Updating stack: {stack_name}")
                    try:
                        self.cf_client.update_stack(
                            StackName=stack_name,
                            TemplateURL=template_url,
                            Parameters=cf_parameters,
                            Capabilities=capabilities or []
                        )
                    except Exception as e:
                        if "No updates are to be performed" in str(e):
                            print(f"‚ÑπÔ∏è  Stack {stack_name} is already up to date")
                            return True
                        raise
            
            if not stack_exists:
                print(f"üöÄ Creating stack: {stack_name}")
                self.cf_client.create_stack(
                    StackName=stack_name,
                    TemplateURL=template_url,
                    Parameters=cf_parameters,
                    Capabilities=capabilities or []
                )
            
            # Wait for stack operation to complete
            return self._wait_for_stack_operation(stack_name)
            
        except Exception as e:
            error_msg = str(e)
            # Handle parameter mismatch errors
            if "do not exist in the template" in error_msg:
                print(f"‚ö†Ô∏è  Parameter mismatch detected - recreating stack: {stack_name}")
                self._delete_stack(stack_name)
                print(f"üöÄ Creating stack: {stack_name}")
                self.cf_client.create_stack(
                    StackName=stack_name,
                    TemplateURL=template_url,
                    Parameters=cf_parameters,
                    Capabilities=capabilities or []
                )
                return self._wait_for_stack_operation(stack_name)
            
            print(f"‚ùå Failed to deploy {stack_name}: {error_msg}")
            return False
    
    def _stack_exists(self, stack_name: str) -> bool:
        """Check if CloudFormation stack exists."""
        try:
            response = self.cf_client.describe_stacks(StackName=stack_name)
            stack_status = response['Stacks'][0]['StackStatus']
            return stack_status not in ['DELETE_COMPLETE']
        except:
            return False
    
    def _needs_recreation(self, stack_name: str, new_parameters: Dict[str, str]) -> bool:
        """Check if stack needs recreation due to parameter changes."""
        try:
            response = self.cf_client.describe_stacks(StackName=stack_name)
            existing_params = {p['ParameterKey']: p['ParameterValue'] 
                             for p in response['Stacks'][0].get('Parameters', [])}
            
            # Check if new parameters exist in the current stack
            new_param_keys = set(new_parameters.keys())
            existing_param_keys = set(existing_params.keys())
            
            # If there are new parameters that don't exist in the stack, we need to recreate
            if new_param_keys - existing_param_keys:
                print(f"‚ÑπÔ∏è  New parameters detected: {new_param_keys - existing_param_keys}")
                return True
                
            return False
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not check stack parameters: {str(e)}")
            return False
    
    def _delete_stack(self, stack_name: str) -> bool:
        """Delete a CloudFormation stack."""
        try:
            print(f"üóëÔ∏è  Deleting stack: {stack_name}")
            self.cf_client.delete_stack(StackName=stack_name)
            
            # Wait for deletion to complete
            print(f"‚è≥ Waiting for stack deletion to complete...")
            while True:
                try:
                    response = self.cf_client.describe_stacks(StackName=stack_name)
                    status = response['Stacks'][0]['StackStatus']
                    if status == 'DELETE_COMPLETE':
                        print(f"‚úÖ Stack {stack_name} deleted successfully")
                        return True
                    elif 'DELETE' in status:
                        print(f"‚è≥ Deletion in progress: {status}")
                        time.sleep(30)
                    else:
                        print(f"‚ö†Ô∏è  Unexpected status during deletion: {status}")
                        time.sleep(30)
                except self.cf_client.exceptions.ClientError as e:
                    if 'does not exist' in str(e):
                        print(f"‚úÖ Stack {stack_name} deleted successfully")
                        return True
                    raise
        except Exception as e:
            print(f"‚ùå Failed to delete stack {stack_name}: {str(e)}")
            return False
    
    def _wait_for_stack_operation(self, stack_name: str) -> bool:
        """Wait for CloudFormation stack operation to complete."""
        print(f"‚è≥ Waiting for stack operation to complete...")
        
        while True:
            try:
                response = self.cf_client.describe_stacks(StackName=stack_name)
                stack_status = response['Stacks'][0]['StackStatus']
                
                if stack_status in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']:
                    print(f"‚úÖ Stack {stack_name} operation completed successfully")
                    return True
                elif stack_status in ['CREATE_FAILED', 'UPDATE_FAILED', 
                                    'ROLLBACK_COMPLETE', 'UPDATE_ROLLBACK_COMPLETE']:
                    print(f"‚ùå Stack {stack_name} operation failed: {stack_status}")
                    self._print_stack_events(stack_name)
                    return False
                elif 'IN_PROGRESS' in stack_status:
                    print(f"‚è≥ Stack status: {stack_status}")
                    time.sleep(30)
                else:
                    print(f"‚ö†Ô∏è  Unknown stack status: {stack_status}")
                    time.sleep(30)
                    
            except Exception as e:
                print(f"‚ùå Error checking stack status: {str(e)}")
                return False
    
    def _print_stack_events(self, stack_name: str):
        """Print recent stack events for debugging."""
        try:
            events = self.cf_client.describe_stack_events(StackName=stack_name)
            print("\nüìã Recent stack events:")
            for event in events['StackEvents'][:5]:  # Show last 5 events
                timestamp = event.get('Timestamp', 'Unknown')
                resource_type = event.get('ResourceType', 'Unknown')
                logical_id = event.get('LogicalResourceId', 'Unknown')
                status = event.get('ResourceStatus', 'Unknown')
                reason = event.get('ResourceStatusReason', '')
                print(f"  {timestamp} - {resource_type} {logical_id}: {status} {reason}")
        except Exception as e:
            print(f"Could not retrieve stack events: {str(e)}")
    
    def get_stack_outputs(self, stack_name: str) -> Dict[str, str]:
        """Get CloudFormation stack outputs."""
        try:
            response = self.cf_client.describe_stacks(StackName=stack_name)
            outputs = response['Stacks'][0].get('Outputs', [])
            return {output['OutputKey']: output['OutputValue'] for output in outputs}
        except Exception as e:
            print(f"‚ùå Failed to get outputs from {stack_name}: {str(e)}")
            return {}


def main():
    """Main deployment orchestration."""
    parser = argparse.ArgumentParser(description='Deploy F1 Data Platform AWS Infrastructure')
    parser.add_argument('--environment', '-e', default='dev', 
                       choices=['dev', 'staging', 'prod'],
                       help='Environment to deploy (default: dev)')
    parser.add_argument('--region', '-r', default='us-east-1',
                       help='AWS region (default: us-east-1)')
    parser.add_argument('--profile', '-p', default=None,
                       help='AWS profile to use')
    parser.add_argument('--project-name', default='f1-data-platform',
                       help='Project name for resource naming')
    parser.add_argument('--monitoring-level', default='minimal',
                       choices=['minimal', 'standard', 'full', 'none'],
                       help='CloudWatch monitoring level: minimal (basic metrics), standard (metrics+logs), full (detailed+alarms), none (no monitoring)')
    parser.add_argument('--skip-foundation', action='store_true',
                       help='Skip foundation stack deployment')
    parser.add_argument('--skip-glue', action='store_true',
                       help='Skip Glue ETL stack deployment')
    parser.add_argument('--skip-athena', action='store_true',
                       help='Skip Athena analytics stack deployment')
    
    args = parser.parse_args()
    
    # Initialize deployer
    deployer = CloudFormationDeployer(region=args.region, profile=args.profile)
    
    # Create deployment bucket (without account ID in name)
    bucket_name = f"f1-platform-cf-templates-{args.environment}"
    deployer.create_deployment_bucket(bucket_name)
    
    # Define deployment configuration
    # Use relative paths from repository root (works in both Windows and Linux)
    gitops_template_dir = Path(__file__).parent.parent.parent / "infrastructure" / "aws" / "cloudformation"
    showcase_template_dir = Path("C:/scripts/showcase-f1-pipeline/config/cloudformation")  # External repo - keep as is for now
    
    stacks = [
        {
            'name': f"{args.project_name}-foundation-{args.environment}",
            'template': '01-data-lake-foundation-fixed.yaml',
            'template_dir': gitops_template_dir,  # Use fixed version
            'parameters': {
                'Environment': args.environment,
                'ProjectName': args.project_name,
                'DataLakeBucketName': f'f1-data-lake',
                'MonitoringLevel': args.monitoring_level
            },
            'capabilities': ['CAPABILITY_NAMED_IAM'],
            'skip': args.skip_foundation
        },
        {
            'name': f"{args.project_name}-glue-etl-{args.environment}",
            'template': '02-glue-etl-jobs.yaml',
            'template_dir': showcase_template_dir,
            'parameters': {
                'Environment': args.environment,
                'ProjectName': args.project_name,
                'MonitoringLevel': args.monitoring_level,
                # These will be populated from foundation stack outputs
                'DataLakeBucket': '',
                'GlueDatabase': '',
                'GlueServiceRoleArn': ''
            },
            'capabilities': [],
            'skip': args.skip_glue,
            'depends_on': f"{args.project_name}-foundation-{args.environment}"
        },
        {
            'name': f"{args.project_name}-athena-analytics-{args.environment}",
            'template': '03-athena-analytics.yaml',
            'template_dir': showcase_template_dir,
            'parameters': {
                'Environment': args.environment,
                'ProjectName': args.project_name,
                'MonitoringLevel': args.monitoring_level,
                # These will be populated from foundation stack outputs
                'DataLakeBucket': '',
                'GlueDatabase': '',
                'AthenaWorkgroup': ''
            },
            'capabilities': [],
            'skip': args.skip_athena,
            'depends_on': f"{args.project_name}-foundation-{args.environment}"
        },
        {
            'name': f"{args.project_name}-access-{args.environment}",
            'template': '04-f1-data-platform-access-role.yaml',
            'template_dir': showcase_template_dir,
            'parameters': {
                'Environment': args.environment,
                'ProjectName': args.project_name,
                # These will be populated from foundation stack outputs
                'DataLakeBucket': '',
                'GlueDatabase': '',
                'AthenaResultsBucket': ''
            },
            'capabilities': ['CAPABILITY_NAMED_IAM'],
            'skip': False,
            'depends_on': f"{args.project_name}-foundation-{args.environment}"
        }
    ]
    
    print(f"üöÄ Starting F1 Data Platform deployment to {args.environment} environment")
    print(f"üìç Region: {args.region}")
    print(f"ü™£ Deployment bucket: {bucket_name}")
    print("=" * 70)
    
    # Deploy stacks in order
    deployed_stacks = {}
    
    for stack_config in stacks:
        if stack_config['skip']:
            print(f"‚è≠Ô∏è  Skipping {stack_config['name']}")
            continue
            
        # Upload template (use stack-specific template_dir)
        template_path = stack_config['template_dir'] / stack_config['template']
        if not template_path.exists():
            print(f"‚ùå Template not found: {template_path}")
            continue
            
        template_url = deployer.upload_template(template_path, bucket_name, "templates/")
        
        # Handle dependencies
        if 'depends_on' in stack_config:
            dependency_stack = stack_config['depends_on']
            if dependency_stack in deployed_stacks:
                outputs = deployed_stacks[dependency_stack]
                # Map foundation outputs to dependent stack parameters
                if 'DataLakeBucket' in stack_config['parameters']:
                    stack_config['parameters']['DataLakeBucket'] = outputs.get('DataLakeBucketName', '')
                if 'GlueDatabase' in stack_config['parameters']:
                    stack_config['parameters']['GlueDatabase'] = outputs.get('GlueDatabaseName', '')
                if 'GlueServiceRoleArn' in stack_config['parameters']:
                    stack_config['parameters']['GlueServiceRoleArn'] = outputs.get('GlueServiceRoleArn', '')
                if 'AthenaWorkgroup' in stack_config['parameters']:
                    stack_config['parameters']['AthenaWorkgroup'] = outputs.get('AthenaWorkgroupName', '')
                if 'AthenaResultsBucket' in stack_config['parameters']:
                    stack_config['parameters']['AthenaResultsBucket'] = outputs.get('AthenaResultsBucketName', '')
            else:
                print(f"‚ùå Dependency {dependency_stack} not found, skipping {stack_config['name']}")
                continue
        
        # Deploy stack
        success = deployer.deploy_stack(
            stack_config['name'],
            template_url,
            stack_config['parameters'],
            stack_config['capabilities']
        )
        
        if success:
            # Store outputs for dependent stacks
            outputs = deployer.get_stack_outputs(stack_config['name'])
            deployed_stacks[stack_config['name']] = outputs
            print(f"üìä Stack outputs: {json.dumps(outputs, indent=2)}")
        else:
            print(f"‚ùå Failed to deploy {stack_config['name']}")
            break
        
        print("-" * 70)
    
    # Summary
    print("\nüéØ Deployment Summary:")
    for stack_name, outputs in deployed_stacks.items():
        print(f"‚úÖ {stack_name}: Successfully deployed")
    
    if len(deployed_stacks) == len([s for s in stacks if not s['skip']]):
        print("\nüéâ F1 Data Platform deployment completed successfully!")
        print("\nNext steps:")
        print("1. Upload Glue ETL scripts to the data lake bucket")
        print("2. Run the Glue crawlers to discover data schemas")
        print("3. Execute the named queries in Athena for analytics")
        print("4. Set up your F1 data extraction pipeline")
    else:
        print("\n‚ö†Ô∏è  Deployment completed with errors. Check the logs above.")


if __name__ == '__main__':
    main()